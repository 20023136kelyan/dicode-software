Below is a **single unified Markdown spec** that covers **both**:

### âœ… **Question Generation**

(Used when the system automatically creates questions from a video)

### âœ… **Question Verification**

(Used when the user manually enters questions and the LLM must check compliance)

This is the **master specification file** your LLM can load to govern both workflows.
It is structured, strict, complete, and aligned with your PDF standards.
You can place it inside your repo under something like:

> `/docs/questionnaire-spec.md`

---

# ğŸ¯ **CROSSAND QUESTIONNAIRE SPECIFICATION**

**For Workplace Video Scenarios â€” Generation + Verification Rules**
**Version:** 1.0
**Frameworks:** DI Code Behavioral Framework + High-Fidelity Behavioral Analytics 
**Purpose:**

* Provide strict rules for *question generation*
* Provide strict rules for *manual question validation*
* Ensure psychometric reliability, behavioral computability, and Gen Z cognitive load compatibility

---

# ---------------------------------------------

# **1. OVERVIEW**

# ---------------------------------------------

This specification standardizes how the system produces and validates questions following a video scenario.

Every question set must adhere to a **2+1 architecture**:

1. **Q1 â€” Behavioral Perception** *(quantitative)*
2. **Q2 â€” Behavioral Intent / Application** *(quantitative)*
3. **Q3 â€” Qualitative Insight** *(optional short text)*

The purpose is to:

* assess how users interpret workplace behaviors
* measure how they would respond
* gather explainable insight for LLM-coded analytics
* ensure questions are computable for longitudinal behavior modeling

---

# ---------------------------------------------

# **2. QUESTION GENERATION RULES (SYSTEM-CREATED)**

# ---------------------------------------------

This section defines how the system must *generate* questions automatically from scenario metadata.

## **2.1 Inputs Required for Generation**

The generator consumes:

### **A. Scenario Metadata**

* competence/behavior tag (DI Code category)
* conflict / event summary
* emotional states
* correct behavior to teach
* characters involved
* workplace setting

### **B. Cinematic Metadata**

Used to adjust phrasing difficulty:

* clarity level of video
* environment type
* lighting accuracy
* camera angles
* motion complexity

### **C. Output Requirements**

The generated questions must be:

* short
* behavior-specific
* computable
* one construct per question
* designed with Gen Z cognitive load in mind

---

## **2.2 Generation Templates**

### **Q1: Behavioral Perception (Quantitative)**

**Definition:** Measures understanding or interpretation of the behavior shown.

**Template Types:**

* â€œHow clearly did you understand {behavior/conflict}?â€
* â€œHow accurately did you perceive {Person A}â€™s emotional state?â€
* â€œHow well did the video show {specific behavior indicator}?â€

**Scale:** 1â€“7 + Don't Know

---

### **Q2: Behavioral Intent / Application (Quantitative)**

**Definition:** Measures likelihood of applying the correct behavior.

**Template Types:**

* â€œHow likely are you to respond with {correct behavior} in a similar situation?â€
* â€œHow confident would you feel handling a situation like this?â€
* â€œHow likely are you to avoid the problematic behavior shown?â€

**Scale:** 1â€“7 + Don't Know

---

### **Q3: Qualitative Insight (Optional Short Text)**

**Definition:** Captures user reasoning.
**Usage:** LLM will code themes for behavioral barriers.

**Template Types:**

* â€œWhat influenced how you interpreted this situation?â€
* â€œWhat would make responding correctly easier or harder?â€
* â€œWhat stood out to you most in this scenario?â€

---

## **2.3 Generation Rules**

The system **must:**

1. Produce questions â‰¤ **18 words**
2. Use simple, familiar language
3. Avoid ambiguous qualifiers (never use â€œoftenâ€, â€œsometimesâ€, â€œusuallyâ€)
4. Never combine two constructs in one question
5. Ensure numeric computability for Q1 & Q2
6. Make questions strictly about the scenario presented
7. Ensure Q3 does not contain evaluative or sensitive content
8. Follow the mandated structure: Q1 â†’ Q2 â†’ (Q3 optional)

If generation fails any rule, regenerate automatically.

---

# ---------------------------------------------

# **3. QUESTION VERIFICATION RULES (USER-CREATED)**

# ---------------------------------------------

When a user manually enters questions, the LLL must verify compliance with the same standards.

## **3.1 Acceptance Criteria**

A manual question set is **Valid** only if:

* Includes Q1 and Q2
* Follows the order Q1 â†’ Q2 â†’ (Q3 optional)
* Q1 and Q2 are quantitative and map to a 7-point scale
* Each question measures **one construct only**
* Language is plain, clear, and Gen Z friendly
* No psychological manipulation or evaluation
* No ambiguous time words
* Questions relate directly to the video
* Q3 is optional but must be qualitative if present

---

## **3.2 Automatic Rejection Conditions**

Reject a question set if:

* More or fewer than 2â€“3 questions
* Q1 or Q2 is non-quantitative
* A question contains multiple constructs
* The wording is ambiguous or too complex
* Questions cannot produce numeric data
* Content is irrelevant to the scenario
* Questions imply personal identity, diagnosis, or sensitive traits
* Q3 is missing but Q1 or Q2 is malformed
* Q3 is quantitative instead of qualitative
* Any question exceeds 18 words
* Any includes qualifiers (â€œoftenâ€, â€œsometimesâ€, â€œtypicallyâ€, â€œregularlyâ€)

---

## **3.3 What the LLM Must Output During Verification**

When verifying, the LLM must output:

```
VALIDITY: Valid / Invalid
REASONS: (list all violations)
SUGGESTED FIX: (provide corrected question set)
```

---

# ---------------------------------------------

# **4. EXAMPLES OF VALID QUESTION SETS**

# ---------------------------------------------

## **Scenario: Interrupting Colleague in a Meeting**

**Q1:** â€œHow clearly did you understand why the interruption occurred?â€
**Q2:** â€œHow likely are you to support the interrupted colleague in a real meeting?â€
**Q3:** â€œWhat made interpreting this situation easier or harder?â€

## **Scenario: Poorly Delivered Feedback**

**Q1:** â€œHow well did you understand the feedback given?â€
**Q2:** â€œHow likely are you to deliver feedback constructively?â€
**Q3:** â€œWhat could make giving such feedback easier?â€

---

# ---------------------------------------------

# **5. FULL CHECKLIST (LLM MUST FOLLOW THIS)**

# ---------------------------------------------

### **5.1 For Every Q1 and Q2**

* â˜ One construct only
* â˜ Quantitative
* â˜ 7-point scale compatibility
* â˜ Clear & plain language
* â˜ â‰¤ 18 words
* â˜ No ambiguous qualifiers
* â˜ Directly tied to video behavior
* â˜ Not about personality traits
* â˜ Not evaluative or moralizing

### **5.2 For Q3**

* â˜ Optional
* â˜ Qualitative
* â˜ Reflection-based
* â˜ No sensitive personal information
* â˜ No psychological diagnosis
* â˜ Not a quantitative question

---

# ---------------------------------------------

# **6. DEVELOPER GUIDANCE**

# ---------------------------------------------

### **6.1 Question Generator Implementation**

* Uses scenario metadata â†’ chooses templates
* Inserts variables {Person A}, {behavior}, {setting}
* Applies clarity/difficulty calibration based on cinematic metadata
* Enforces all rules before output

### **6.2 Validation Module Implementation**

* Loads this MD
* Evaluates each user question set line-by-line
* Rejects with specific reasons
* Suggests corrected questions
* Logs violations for analytics
* Ensures consistency across all user-created content

---

# ---------------------------------------------

# **7. ETHICAL REQUIREMENTS**

# ---------------------------------------------

A question is invalid if it:

* pressures users
* shames or evaluates performance
* infers race, ethnicity, sexuality, religion
* asks about medical or mental health
* manipulates emotional response
* violates privacy-by-design principles 

The questionnaireâ€™s purpose is to teach behavior â€” not judge identity.

---

# ---------------------------------------------

# **8. CONCLUSION**

# ---------------------------------------------

This specification ensures:

* Consistent behavioral data
* Psychometric validity
* Scalable longitudinal tracking
* Ethical, safe behavioral learning
* Smooth UX for Gen Z users
* Interchangeability between generated and manually entered questions

Your LLM can now **both generate new questions** AND **verify user-submitted questions** with complete reliability.

---

# âœ”ï¸ If you want, I can also provide:

### ğŸ”§ A JSON schema for generator input/output

### ğŸ§  A prompt template for the LLM to follow this MD perfectly

### ğŸ—ï¸ A Firestore schema for storing questionnaire results

### ğŸ“± UI text + UX flows for question entry and validation

Just tell me what you want next.
